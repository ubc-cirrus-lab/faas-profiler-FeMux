{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../output/scalability_data/mc_horizontal_scalability_data.pickle\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_elements = []\n",
    "all_rps = {1: 20, 2: 40, 3: 60, 4: 80, 5: 100}\n",
    "\n",
    "pod_count_response_times = {}\n",
    "colors = {1: \"blue\", 2: \"orange\", 3: \"green\", 4: \"red\", 5: \"purple\"}\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    forecaster_count = row[\"POD_COUNT\"]\n",
    "    response_times = row[\"RESPONSE_TIMES\"][-1000:]\n",
    "    rps = int(row[\"RPS\"])\n",
    "    # all_rps.add(rps)\n",
    "    median_response_time_per_iter = np.median(response_times)\n",
    "    avg_response_time_per_iter = np.mean(response_times)\n",
    "    p95_response_time_per_iter = np.percentile(response_times, 99)\n",
    "\n",
    "    chunked_p95_times = []\n",
    "    chunked_med_times = []\n",
    "\n",
    "    chunk_size = 200\n",
    "\n",
    "    for i in range(0, len(response_times), chunk_size):\n",
    "        chunked_p95_times.append(np.percentile(response_times[i:i+chunk_size], 99))\n",
    "        chunked_med_times.append(np.median(response_times[i:i+chunk_size]))\n",
    "    \n",
    "    mean_p95 = np.mean(chunked_p95_times)\n",
    "    sem_p95 = stats.sem(chunked_p95_times)\n",
    "    confidence = 0.95\n",
    "\n",
    "    confidence_interval_for_p95 = stats.t.interval(confidence, len(chunked_p95_times)-1, loc=mean_p95, scale=sem_p95)\n",
    "\n",
    "    mean_med = np.mean(chunked_med_times)\n",
    "    sem_med = stats.sem(chunked_med_times)\n",
    "\n",
    "    confidence_interval_for_median = stats.t.interval(confidence, len(chunked_med_times)-1, loc=mean_med, scale=sem_med)\n",
    "    \n",
    "    if forecaster_count not in pod_count_response_times:\n",
    "        pod_count_response_times[forecaster_count] = []\n",
    "    pod_count_response_times[forecaster_count].append([median_response_time_per_iter*1000, \n",
    "                                                       confidence_interval_for_median[0]*1000, \n",
    "                                                       confidence_interval_for_median[1]*1000, \n",
    "                                                       p95_response_time_per_iter*1000,\n",
    "                                                       confidence_interval_for_p95[0]*1000, \n",
    "                                                       confidence_interval_for_p95[1]*1000])\n",
    "print(pod_count_response_times)\n",
    "cur_rps = []\n",
    "cur_resp_med = []\n",
    "cur_lci_med = []\n",
    "cur_uci_med = []\n",
    "\n",
    "cur_resp_p95 = []\n",
    "cur_lci_p95 = []\n",
    "cur_uci_p95 = []\n",
    "for count, response_times_and_ci in pod_count_response_times.items():\n",
    "\n",
    "    for resp_med, lci_med, uci_med, resp_p95, lci_p95, uci_p95 in response_times_and_ci:\n",
    "        cur_rps.append(all_rps[count])\n",
    "        cur_lci_med.append(lci_med)\n",
    "        cur_uci_med.append(uci_med)\n",
    "        cur_resp_med.append(resp_med)\n",
    "\n",
    "        cur_lci_p95.append(lci_p95)\n",
    "        cur_uci_p95.append(uci_p95)\n",
    "        cur_resp_p95.append(resp_p95)\n",
    "\n",
    "ax1.plot(cur_rps, cur_resp_med, 'o--', label=\"p50\", color=\"blue\")\n",
    "ax1.plot(cur_rps, cur_resp_p95, 'o--', label=\"p95\", color=\"orange\")\n",
    "print(cur_rps, cur_resp_med)\n",
    "# plot the confidence intervals as vertical lines\n",
    "ax1.errorbar(cur_rps, cur_resp_med, yerr=[np.array(cur_resp_med)-np.array(cur_lci_med), np.array(cur_uci_med)-np.array(cur_resp_med)], fmt='o', color=\"blue\",  alpha=0.5)\n",
    "ax1.errorbar(cur_rps, cur_resp_p95, yerr=[np.array(cur_resp_p95)-np.array(cur_lci_p95), np.array(cur_uci_p95)-np.array(cur_resp_p95)], fmt='o', color=\"orange\",  alpha=0.5)\n",
    "# print(np.array(cur_resp_med)-np.array(cur_lci_med), np.array(cur_uci_med)-np.array(cur_resp_med))\n",
    "ax1.plot(cur_rps, cur_lci_med, '_', color=\"blue\")\n",
    "ax1.plot(cur_rps, cur_uci_med, '_', color=\"blue\")\n",
    "\n",
    "ax1.plot(cur_rps, cur_lci_p95, '_', color=\"orange\")\n",
    "ax1.plot(cur_rps, cur_uci_p95, '_', color=\"orange\")\n",
    "\n",
    "ax1.set_xticks(list(all_rps.values()))\n",
    "\n",
    "ax1.set_xlabel(\"Forecasting RPS\")\n",
    "ax1.set_ylabel(\"Forecasting Latency (ms)\")\n",
    "\n",
    "# create a second x-axis at the top of the plot\n",
    "ax2 = ax1.twiny()\n",
    "\n",
    "# calculate the new x-values for the top x-axis\n",
    "new_xticks = [value*60 for value in all_rps.values()]\n",
    "new_xticklabels = [value for value in all_rps.values()]\n",
    "\n",
    "# set the xticks and xticklabels for the top x-axis\n",
    "ax2.set_xticks(new_xticks)\n",
    "ax2.set_xticklabels(new_xticks)\n",
    "ax2.set_xlabel(\"Number of Applications\")\n",
    "\n",
    "# calculate the padding for the x-axis limits\n",
    "padding = (max(all_rps.values()) - min(all_rps.values())) * 0.05\n",
    "\n",
    "# set the limits of both axes to be the same with padding\n",
    "ax1.set_xlim([min(all_rps.values()) - padding, max(all_rps.values()) + padding])\n",
    "ax2.set_xlim([min(new_xticks) - padding*60, max(new_xticks) + padding*60])\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "# plt.legend()\n",
    "plt.show()\n",
    "fig.savefig(\"../output/scalability_data/hsp.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create dummy forecasting data to maintain continuity\n",
    "# dummy_forecasting_data_5 = pd.DataFrame({\"FORECASTER\": [\"5\"]*5 ,\t\n",
    "#                                          \"POD_COUNT\": [i for i in range(1, 6)],\n",
    "#                                          \"RPS\": [i*20 for i in range(1, 6)],\n",
    "#                                         \"RESPONSE_TIMES\": [[0]*5000]*5})\n",
    "# dummy_forecasting_data_1 = pd.DataFrame({\"FORECASTER\": [\"1\"]*5 ,\t\n",
    "#                                          \"POD_COUNT\": [i for i in range(1, 6)],\n",
    "#                                          \"RPS\": [i*20 for i in range(1, 6)],\n",
    "#                                         \"RESPONSE_TIMES\": [[0]*5000]*5})\n",
    "# dummy_forecasting_data_5.to_pickle(\"../output/per_forecaster_data/5_horizontal_scalability_data.pickle\")\n",
    "# dummy_forecasting_data_1.to_pickle(\"../output/per_forecaster_data/10_horizontal_scalability_data.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_map = {'S': 22, 'A': 22, 'H': 1, 'E': 8, 'M': 534, '1': 15, '5': 393, 'F': 2}\n",
    "weight_map = {'S': 22, 'A': 22, 'H': 1, 'E': 8, 'M': 534, '1': 15, '5': 393, 'F': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all dataframes in per_forecaster_data folder\n",
    "df = pd.DataFrame()\n",
    "# forecasters = [\"5\", \"10\", \"ar\", \"es\", \"holt\", \"mc\", \"setar\", \"fft\"]\n",
    "forecasters = [\"5\", \"10\", \"ar\", \"es\", \"holt\", \"mc\", \"setar\", \"fft\"]\n",
    "for i in range(len(forecasters)):\n",
    "    df = pd.concat([df, pd.read_pickle(f\"../output/per_forecaster_data_v2/{forecasters[i]}_horizontal_scalability_data.pickle\")], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference latency\n",
    "latency_map = {}\n",
    "buckets = [20, 40, 60, 80, 100]\n",
    "median_latency = {}\n",
    "mean_latency = {}\n",
    "nn_latency = {}\n",
    "median_latency_ci = {}\n",
    "nn_latency_ci = {}\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # for bucket in buckets:\n",
    "    apps = row[\"FORECASTER\"][0] + \"_\" + str(row[\"RPS\"])\n",
    "    if apps not in latency_map:\n",
    "        latency_map[apps] = []\n",
    "    if \"1\" == apps[0] or \"5\" == apps[0]:\n",
    "        latency_map[apps].extend([0]*len(row[\"RESPONSE_TIMES\"][-1000:]))\n",
    "    else:\n",
    "        latency_map[apps].extend(row[\"RESPONSE_TIMES\"][-1000:])\n",
    "for key, val in latency_map.items():\n",
    "        latency_map[key] = val * weight_map[key.split(\"_\")[0]]\n",
    "        if \"F\" in key:\n",
    "            print(len(latency_map[key]))\n",
    "\n",
    "cur_uci_med = []\n",
    "cur_lci_med = []\n",
    "cur_resp_med = []\n",
    "\n",
    "cur_uci_p95 = []\n",
    "cur_lci_p95 = []\n",
    "cur_resp_p95 = []\n",
    "overall_median_latency = {}\n",
    "overall_nn_latency = {}\n",
    "\n",
    "for bucket in buckets:\n",
    "    same_bucket_list = []\n",
    "    for key, val in latency_map.items():\n",
    "        if f\"{bucket}\" in key:\n",
    "            same_bucket_list += val\n",
    "\n",
    "    # shuffle same_bucket_list to avoid any ordering bias and again get the list\n",
    "    same_bucket_list.sort()\n",
    "    same_bucket_list = np.random.permutation(same_bucket_list).tolist()\n",
    "\n",
    "    chunked_p95_times = []\n",
    "    chunked_med_times = []\n",
    "\n",
    "    chunk_size = 1000\n",
    "\n",
    "    for i in range(0, len(same_bucket_list), chunk_size):\n",
    "        chunked_p95_times.append(np.percentile(same_bucket_list[i:i+chunk_size], 99))\n",
    "        chunked_med_times.append(np.median(same_bucket_list[i:i+chunk_size]))\n",
    "\n",
    "    print(len(chunked_med_times), len(chunked_p95_times))\n",
    "    median = np.mean(chunked_med_times)\n",
    "    median_latency[bucket] = median\n",
    "    overall_median_latency[bucket] = np.mean(same_bucket_list)\n",
    "\n",
    "    # mean_latency[bucket] = np.mean(same_bucket_list)\n",
    "    nn = np.percentile([elem for elem in chunked_p95_times if elem != None], 99)\n",
    "    nn_latency[bucket] = nn\n",
    "    overall_nn_latency[bucket] = np.percentile(same_bucket_list, 99)\n",
    "\n",
    "    # Calculate the standard error of the mean\n",
    "    sem_p95 = stats.sem(chunked_p95_times)\n",
    "    sem_med = stats.sem(chunked_med_times)\n",
    "\n",
    "    # Calculate the confidence intervals\n",
    "    ci_med = stats.t.interval(0.95, len(chunked_med_times)-1, loc=np.median(same_bucket_list), scale=sem_med)\n",
    "    median_latency_ci[bucket] = ci_med\n",
    "    cur_uci_med.append(ci_med[1]*1000)\n",
    "    cur_lci_med.append(ci_med[0]*1000)\n",
    "    cur_resp_med.append(median*1000)\n",
    "\n",
    "    ci_nn = stats.t.interval(0.95, len(chunked_p95_times)-1, loc=np.percentile(same_bucket_list, 99), scale=sem_p95)\n",
    "    nn_latency_ci[bucket] = ci_nn\n",
    "    cur_uci_p95.append(ci_nn[1]*1000)\n",
    "    cur_lci_p95.append(ci_nn[0]*1000)\n",
    "    cur_resp_p95.append(nn*1000)\n",
    "\n",
    "# Add error bars to the plot\n",
    "ax1.errorbar(buckets, [overall_median_latency[bucket] * 1000 for bucket in buckets], \n",
    "             yerr=[((upper-lower) * 1000) for lower, upper in median_latency_ci.values()], \n",
    "             marker='.', capsize=5, color='blue')\n",
    "\n",
    "ax1.errorbar(buckets, [overall_nn_latency[bucket] * 1000 for bucket in buckets], \n",
    "             yerr=[((upper-lower) * 1000) for lower, upper in nn_latency_ci.values()],\n",
    "             marker='.', capsize=5, color='orange')\n",
    "\n",
    "ax1.set_xlabel(\"Forecasting RPS\")\n",
    "ax1.set_ylabel(\"Forecasting Latency (ms)\")\n",
    "\n",
    "ax1.set_xticks(buckets)\n",
    "ax1.set_xticklabels(buckets)\n",
    "\n",
    "\n",
    "# create a second x-axis at the top of the plot\n",
    "ax2 = ax1.twiny()\n",
    "\n",
    "# calculate the new x-values for the top x-axis\n",
    "new_xticks = [value*60 for value in buckets]\n",
    "new_xticklabels = [value for value in buckets]\n",
    "\n",
    "# set the xticks and xticklabels for the top x-axis\n",
    "ax2.set_xticks(new_xticks)\n",
    "ax2.set_xticklabels(new_xticks)\n",
    "ax2.set_xlabel(\"Number of Applications\")\n",
    "\n",
    "# calculate the padding for the x-axis limits\n",
    "padding = (max(buckets) - min(buckets)) * 0.05\n",
    "\n",
    "# set the limits of both axes to be the same with padding\n",
    "ax1.set_xlim([min(buckets) - padding, max(buckets) + padding])\n",
    "ax2.set_xlim([min(new_xticks) - padding*60, max(new_xticks) + padding*60])\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "\n",
    "ax1.set_ylim([0, 30])\n",
    "\n",
    "fig.savefig(\"../output/scalability_data/hsp_inference_latency.pdf\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
